{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction aux Auto-encodeurs avec TensorFlow/Keras\n",
    "\n",
    "## Objectifs de ce notebook\n",
    "\n",
    "Dans ce notebook, nous allons explorer les auto-encodeurs, une architecture fondamentale du deep learning pour l'apprentissage non supervis√© :\n",
    "\n",
    "1. **Auto-encodeur Dense (Vanilla)** - L'architecture de base\n",
    "2. **Auto-encodeur Convolutif** - Optimis√© pour les images\n",
    "3. **Denoising Auto-encodeur** - Robuste au bruit\n",
    "4. **Visualisation de l'espace latent** - Comprendre les repr√©sentations apprises\n",
    "\n",
    "Nous utiliserons le dataset **Fashion MNIST** pour maintenir la coh√©rence avec les autres notebooks.\n",
    "\n",
    "## Qu'est-ce qu'un Auto-encodeur ?\n",
    "\n",
    "Un auto-encodeur est un r√©seau de neurones qui apprend √† **compresser** puis **reconstruire** ses propres entr√©es :\n",
    "\n",
    "```\n",
    "Input (784) ‚Üí Encoder ‚Üí Latent Space (32) ‚Üí Decoder ‚Üí Output (784)\n",
    "```\n",
    "\n",
    "### Applications pratiques :\n",
    "- üé® **Compression d'images**\n",
    "- üîç **D√©tection d'anomalies**\n",
    "- üßπ **D√©bruitage d'images**\n",
    "- üé≠ **G√©n√©ration de nouvelles donn√©es**\n",
    "- üìä **R√©duction de dimensionnalit√©**\n",
    "- üîê **Extraction de features pour d'autres t√¢ches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioth√®ques principales\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# Utilitaires\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuration GPU\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU(s) d√©tect√©(s): {len(gpus)} - Croissance m√©moire activ√©e\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  Aucun GPU d√©tect√© - Utilisation du CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Configuration GPU: {e}\")\n",
    "    print(\"Utilisation du CPU par d√©faut\")\n",
    "\n",
    "# Configuration graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproductibilit√©\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"\\nüì¶ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üì¶ Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et pr√©paration des donn√©es\n",
    "\n",
    "### 2.1 Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Noms des classes\n",
    "class_names = ['T-shirt/top', 'Pantalon', 'Pull', 'Robe', 'Manteau',\n",
    "               'Sandale', 'Chemise', 'Basket', 'Sac', 'Bottine']\n",
    "\n",
    "print(f\"üìä Forme des donn√©es d'entra√Ænement: {X_train.shape}\")\n",
    "print(f\"üìä Forme des donn√©es de test: {X_test.shape}\")\n",
    "print(f\"üìä Plage des valeurs: [{X_train.min()}, {X_train.max()}]\")\n",
    "print(f\"\\nüí° Pour les auto-encodeurs, nous n'utiliserons PAS les labels (apprentissage non supervis√©)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualisation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
    "fig.suptitle('√âchantillon du dataset Fashion MNIST', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i], cmap='gray')\n",
    "    ax.set_title(f'{class_names[y_train[i]]}', fontsize=11)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalisation des donn√©es\n",
    "\n",
    "**Important** : La normalisation est cruciale pour les auto-encodeurs :\n",
    "- Valeurs entre 0 et 1\n",
    "- Facilite la convergence\n",
    "- Permet d'utiliser une fonction d'activation sigmoid en sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation\n",
    "X_train_normalized = X_train.astype('float32') / 255.0\n",
    "X_test_normalized = X_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"‚úÖ Normalisation effectu√©e\")\n",
    "print(f\"Nouvelles valeurs - Min: {X_train_normalized.min():.2f}, Max: {X_train_normalized.max():.2f}\")\n",
    "print(f\"Moyenne: {X_train_normalized.mean():.4f}, √âcart-type: {X_train_normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Cr√©ation d'un set de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation\n",
    "X_val = X_train_normalized[50000:]\n",
    "X_train_final = X_train_normalized[:50000]\n",
    "\n",
    "# Labels pour l'analyse (pas pour l'entra√Ænement)\n",
    "y_val = y_train[50000:]\n",
    "y_train_final = y_train[:50000]\n",
    "\n",
    "print(f\"üì¶ Donn√©es d'entra√Ænement: {X_train_final.shape[0]:,} exemples\")\n",
    "print(f\"üì¶ Donn√©es de validation: {X_val.shape[0]:,} exemples\")\n",
    "print(f\"üì¶ Donn√©es de test: {X_test_normalized.shape[0]:,} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Callback personnalis√© pour visualisation\n",
    "\n",
    "Ce callback affiche les reconstructions en temps r√©el pendant l'entra√Ænement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderVisualizationCallback(Callback):\n",
    "    \"\"\"\n",
    "    Callback pour visualiser les reconstructions et les m√©triques en temps r√©el.\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_data, n_images=8):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.n_images = n_images\n",
    "        # S√©lection d'images pour visualisation\n",
    "        self.test_images = validation_data[:n_images]\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Enregistrement des m√©triques\n",
    "        self.epochs.append(epoch + 1)\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "        # Effacement et redessin\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Cr√©ation de la figure\n",
    "        fig = plt.figure(figsize=(20, 8))\n",
    "        gs = fig.add_gridspec(3, self.n_images + 1, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Graphique de la loss (prend 2 colonnes)\n",
    "        ax_loss = fig.add_subplot(gs[:, 0])\n",
    "        ax_loss.plot(self.epochs, self.loss, 'o-', label='Loss Train', \n",
    "                    linewidth=2.5, markersize=8, color='#2E86AB')\n",
    "        ax_loss.plot(self.epochs, self.val_loss, 's-', label='Loss Validation', \n",
    "                    linewidth=2.5, markersize=8, color='#A23B72')\n",
    "        ax_loss.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "        ax_loss.set_ylabel('Loss (MSE)', fontsize=12, fontweight='bold')\n",
    "        ax_loss.set_title('√âvolution de la Loss', fontsize=14, fontweight='bold')\n",
    "        ax_loss.legend(fontsize=10)\n",
    "        ax_loss.grid(alpha=0.3)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        reconstructed = self.model.predict(self.test_images, verbose=0)\n",
    "        \n",
    "        # Images originales\n",
    "        for i in range(self.n_images):\n",
    "            ax = fig.add_subplot(gs[0, i + 1])\n",
    "            img = self.test_images[i].reshape(28, 28)\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            if i == 0:\n",
    "                ax.set_ylabel('Original', fontsize=11, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Images reconstruites\n",
    "        for i in range(self.n_images):\n",
    "            ax = fig.add_subplot(gs[1, i + 1])\n",
    "            img = reconstructed[i].reshape(28, 28)\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            if i == 0:\n",
    "                ax.set_ylabel('Reconstruit', fontsize=11, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        # Diff√©rence (erreur)\n",
    "        for i in range(self.n_images):\n",
    "            ax = fig.add_subplot(gs[2, i + 1])\n",
    "            original = self.test_images[i].reshape(28, 28)\n",
    "            reconstructed_img = reconstructed[i].reshape(28, 28)\n",
    "            diff = np.abs(original - reconstructed_img)\n",
    "            im = ax.imshow(diff, cmap='hot', vmin=0, vmax=1)\n",
    "            if i == 0:\n",
    "                ax.set_ylabel('Erreur', fontsize=11, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        plt.suptitle(f'Auto-encodeur - Epoch {epoch + 1}/{self.params[\"epochs\"]}', \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        plt.show()\n",
    "        \n",
    "        # Affichage textuel\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']}\")\n",
    "        print(f\"Loss: {logs.get('loss'):.6f} - Val Loss: {logs.get('val_loss'):.6f}\")\n",
    "        \n",
    "        # Calcul de l'erreur moyenne de reconstruction\n",
    "        mse = np.mean((self.test_images - reconstructed) ** 2)\n",
    "        print(f\"MSE moyenne sur √©chantillon: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 1 : AUTO-ENCODEUR DENSE (VANILLA)\n",
    "\n",
    "## 4. Architecture de l'auto-encodeur dense\n",
    "\n",
    "### Principe de fonctionnement\n",
    "\n",
    "Un auto-encodeur dense se compose de deux parties :\n",
    "\n",
    "1. **Encoder** : Compresse l'entr√©e vers une repr√©sentation latente\n",
    "   ```\n",
    "   Input (784) ‚Üí Dense(256) ‚Üí Dense(128) ‚Üí Latent (32)\n",
    "   ```\n",
    "\n",
    "2. **Decoder** : Reconstruit l'image √† partir de la repr√©sentation latente\n",
    "   ```\n",
    "   Latent (32) ‚Üí Dense(128) ‚Üí Dense(256) ‚Üí Output (784)\n",
    "   ```\n",
    "\n",
    "### Fonction de perte\n",
    "\n",
    "Pour les auto-encodeurs, on utilise typiquement :\n",
    "- **MSE (Mean Squared Error)** : Mesure la diff√©rence pixel par pixel\n",
    "- **Binary Crossentropy** : Alternative pour images normalis√©es [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pr√©paration des donn√©es pour l'auto-encodeur dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplatissement des images\n",
    "X_train_flat = X_train_final.reshape(-1, 28 * 28)\n",
    "X_val_flat = X_val.reshape(-1, 28 * 28)\n",
    "X_test_flat = X_test_normalized.reshape(-1, 28 * 28)\n",
    "\n",
    "print(f\"‚úÖ Forme apr√®s aplatissement: {X_train_flat.shape}\")\n",
    "print(f\"   Chaque image est un vecteur de {28*28} valeurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Construction de l'auto-encodeur dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_autoencoder(latent_dim=32):\n",
    "    \"\"\"\n",
    "    Cr√©e un auto-encodeur dense avec une dimension latente configurable.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimension de l'espace latent (bottleneck)\n",
    "    \n",
    "    Returns:\n",
    "        autoencoder: Mod√®le complet\n",
    "        encoder: Partie encoder seule\n",
    "        decoder: Partie decoder seule\n",
    "    \"\"\"\n",
    "    # === ENCODER ===\n",
    "    encoder_input = layers.Input(shape=(784,), name='encoder_input')\n",
    "    x = layers.Dense(256, activation='relu', name='encoder_dense_1')(encoder_input)\n",
    "    x = layers.Dense(128, activation='relu', name='encoder_dense_2')(x)\n",
    "    latent = layers.Dense(latent_dim, activation='relu', name='latent_space')(x)\n",
    "    \n",
    "    encoder = Model(encoder_input, latent, name='encoder')\n",
    "    \n",
    "    # === DECODER ===\n",
    "    decoder_input = layers.Input(shape=(latent_dim,), name='decoder_input')\n",
    "    x = layers.Dense(128, activation='relu', name='decoder_dense_1')(decoder_input)\n",
    "    x = layers.Dense(256, activation='relu', name='decoder_dense_2')(x)\n",
    "    decoder_output = layers.Dense(784, activation='sigmoid', name='decoder_output')(x)\n",
    "    \n",
    "    decoder = Model(decoder_input, decoder_output, name='decoder')\n",
    "    \n",
    "    # === AUTOENCODER COMPLET ===\n",
    "    autoencoder_input = layers.Input(shape=(784,), name='autoencoder_input')\n",
    "    encoded = encoder(autoencoder_input)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = Model(autoencoder_input, decoded, name='autoencoder')\n",
    "    \n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# Cr√©ation du mod√®le\n",
    "latent_dim = 32\n",
    "dense_autoencoder, dense_encoder, dense_decoder = create_dense_autoencoder(latent_dim)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ARCHITECTURE DE L'AUTO-ENCODEUR DENSE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüîπ ENCODER:\")\n",
    "dense_encoder.summary()\n",
    "print(\"\\nüîπ DECODER:\")\n",
    "dense_decoder.summary()\n",
    "print(\"\\nüîπ AUTOENCODER COMPLET:\")\n",
    "dense_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualisation de l'architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des param√®tres\n",
    "encoder_params = dense_encoder.count_params()\n",
    "decoder_params = dense_decoder.count_params()\n",
    "total_params = dense_autoencoder.count_params()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# R√©partition des param√®tres\n",
    "components = ['Encoder', 'Decoder']\n",
    "params = [encoder_params, decoder_params]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "bars = ax1.bar(components, params, color=colors, edgecolor='black', linewidth=2, width=0.6)\n",
    "ax1.set_ylabel('Nombre de param√®tres', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('R√©partition des param√®tres', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, param in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{param:,}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Diagramme du flux de donn√©es\n",
    "ax2.axis('off')\n",
    "ax2.text(0.5, 0.9, 'FLUX DE DONN√âES', ha='center', fontsize=16, fontweight='bold')\n",
    "ax2.text(0.5, 0.75, 'Input (784 dims)', ha='center', fontsize=12, \n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', edgecolor='black', linewidth=2))\n",
    "ax2.arrow(0.5, 0.70, 0, -0.08, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
    "ax2.text(0.5, 0.55, 'Dense(256) + ReLU', ha='center', fontsize=11, \n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=1.5))\n",
    "ax2.arrow(0.5, 0.50, 0, -0.08, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
    "ax2.text(0.5, 0.35, 'Dense(128) + ReLU', ha='center', fontsize=11, \n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black', linewidth=1.5))\n",
    "ax2.arrow(0.5, 0.30, 0, -0.08, head_width=0.05, head_length=0.02, fc='black', ec='black')\n",
    "ax2.text(0.5, 0.15, f'LATENT ({latent_dim} dims)', ha='center', fontsize=12, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='yellow', edgecolor='red', linewidth=2))\n",
    "ax2.text(0.1, 0.15, '‚Üê ENCODER', ha='center', fontsize=10, color='blue', fontweight='bold')\n",
    "ax2.text(0.9, 0.15, 'DECODER ‚Üí', ha='center', fontsize=10, color='red', fontweight='bold')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä TOTAL PARAM√àTRES : {total_params:,}\")\n",
    "print(f\"   Taux de compression : 784 ‚Üí {latent_dim} (√ó {784/latent_dim:.1f})\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Compilation du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "dense_autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',  # Ou 'mse' pour Mean Squared Error\n",
    "    metrics=['mse']  # On suit aussi la MSE\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Auto-encodeur compil√©\")\n",
    "print(f\"   Optimiseur: Adam (lr=0.001)\")\n",
    "print(f\"   Loss: Binary Crossentropy\")\n",
    "print(f\"   M√©trique: MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Entra√Ænement de l'auto-encodeur dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du callback de visualisation\n",
    "viz_callback = AutoencoderVisualizationCallback(X_val_flat, n_images=8)\n",
    "\n",
    "print(\"üöÄ D√©but de l'entra√Ænement de l'auto-encodeur DENSE...\\n\")\n",
    "\n",
    "# Entra√Ænement\n",
    "# Note: Pour l'auto-encodeur, X = Y (on reconstruit l'entr√©e)\n",
    "history_dense = dense_autoencoder.fit(\n",
    "    X_train_flat, X_train_flat,  # Input = Output\n",
    "    validation_data=(X_val_flat, X_val_flat),\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    callbacks=[viz_callback],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 √âvaluation de l'auto-encodeur dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur le set de test\n",
    "test_loss, test_mse = dense_autoencoder.evaluate(X_test_flat, X_test_flat, verbose=0)\n",
    "\n",
    "# Reconstruction d'exemples\n",
    "n_examples = 10\n",
    "test_sample = X_test_flat[:n_examples]\n",
    "reconstructed = dense_autoencoder.predict(test_sample, verbose=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä R√âSULTATS - AUTO-ENCODEUR DENSE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Loss sur le test (Binary Crossentropy): {test_loss:.6f}\")\n",
    "print(f\"MSE sur le test: {test_mse:.6f}\")\n",
    "print(f\"RMSE sur le test: {np.sqrt(test_mse):.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualisation d√©taill√©e\n",
    "fig, axes = plt.subplots(3, n_examples, figsize=(20, 6))\n",
    "fig.suptitle('Reconstruction - Auto-encodeur Dense', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(n_examples):\n",
    "    # Original\n",
    "    axes[0, i].imshow(test_sample[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Original', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Reconstruit\n",
    "    axes[1, i].imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Reconstruit', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Diff√©rence\n",
    "    diff = np.abs(test_sample[i].reshape(28, 28) - reconstructed[i].reshape(28, 28))\n",
    "    im = axes[2, i].imshow(diff, cmap='hot', vmin=0, vmax=1)\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel('Erreur', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # MSE pour cette image\n",
    "    mse_img = np.mean((test_sample[i] - reconstructed[i]) ** 2)\n",
    "    axes[2, i].set_title(f'MSE: {mse_img:.4f}', fontsize=9)\n",
    "\n",
    "# Colorbar\n",
    "fig.colorbar(im, ax=axes[2, :], orientation='horizontal', pad=0.05, fraction=0.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Visualisation de l'espace latent\n",
    "\n",
    "L'espace latent est la repr√©sentation compress√©e apprise par l'encoder. Visualisons-le avec t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage du set de test\n",
    "print(\"‚è≥ Encodage des donn√©es de test...\")\n",
    "latent_representations = dense_encoder.predict(X_test_flat, verbose=0)\n",
    "print(f\"‚úÖ Forme de l'espace latent: {latent_representations.shape}\")\n",
    "\n",
    "# R√©duction avec t-SNE pour visualisation 2D\n",
    "print(\"‚è≥ R√©duction de dimension avec t-SNE (cela peut prendre 1-2 minutes)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "latent_2d = tsne.fit_transform(latent_representations[:5000])  # Sur 5000 exemples\n",
    "print(\"‚úÖ t-SNE termin√©\")\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Visualisation color√©e par classe\n",
    "scatter = ax1.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
    "                     c=y_test[:5000], cmap='tab10', \n",
    "                     alpha=0.6, s=10, edgecolors='none')\n",
    "ax1.set_title('Espace latent (color√© par classe)\\nAuto-encodeur Dense', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('Dimension 2', fontsize=12)\n",
    "cbar = plt.colorbar(scatter, ax=ax1, ticks=range(10))\n",
    "cbar.set_label('Classe', fontsize=11)\n",
    "cbar.ax.set_yticklabels(class_names, fontsize=8)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Distribution par classe\n",
    "for i in range(10):\n",
    "    mask = y_test[:5000] == i\n",
    "    ax2.scatter(latent_2d[mask, 0], latent_2d[mask, 1], \n",
    "               label=class_names[i], alpha=0.6, s=15)\n",
    "ax2.set_title('Espace latent (s√©paration par classe)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Dimension 1', fontsize=12)\n",
    "ax2.set_ylabel('Dimension 2', fontsize=12)\n",
    "ax2.legend(loc='best', fontsize=9, ncol=2)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° INTERPR√âTATION:\")\n",
    "print(\"   - Les points proches dans l'espace latent repr√©sentent des images similaires\")\n",
    "print(\"   - Un bon auto-encodeur groupe naturellement les classes similaires\")\n",
    "print(\"   - On peut voir des clusters m√™me sans supervision !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 2 : AUTO-ENCODEUR CONVOLUTIF\n",
    "\n",
    "## 5. Architecture de l'auto-encodeur convolutif\n",
    "\n",
    "### Pourquoi utiliser des convolutions ?\n",
    "\n",
    "Les auto-encodeurs convolutifs sont plus adapt√©s aux images car ils :\n",
    "- ‚úÖ **Pr√©servent la structure spatiale**\n",
    "- ‚úÖ **Utilisent moins de param√®tres**\n",
    "- ‚úÖ **Capturent mieux les features locales**\n",
    "- ‚úÖ **Produisent des reconstructions plus nettes**\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Encoder:\n",
    "Input (28√ó28√ó1) ‚Üí Conv2D(32) + MaxPool ‚Üí Conv2D(64) + MaxPool ‚Üí Flatten ‚Üí Dense(latent_dim)\n",
    "\n",
    "Decoder:\n",
    "Dense ‚Üí Reshape ‚Üí Conv2DTranspose(64) ‚Üí Conv2DTranspose(32) ‚Üí Conv2D(1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Pr√©paration des donn√©es pour CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de la dimension des canaux\n",
    "X_train_cnn = X_train_final.reshape(-1, 28, 28, 1)\n",
    "X_val_cnn = X_val.reshape(-1, 28, 28, 1)\n",
    "X_test_cnn = X_test_normalized.reshape(-1, 28, 28, 1)\n",
    "\n",
    "print(f\"‚úÖ Forme pour CNN: {X_train_cnn.shape}\")\n",
    "print(f\"   Format: (nombre_images, hauteur, largeur, canaux)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Construction de l'auto-encodeur convolutif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convolutional_autoencoder(latent_dim=64):\n",
    "    \"\"\"\n",
    "    Cr√©e un auto-encodeur convolutif.\n",
    "    \n",
    "    Args:\n",
    "        latent_dim: Dimension de l'espace latent\n",
    "    \n",
    "    Returns:\n",
    "        autoencoder, encoder, decoder\n",
    "    \"\"\"\n",
    "    # === ENCODER ===\n",
    "    encoder_input = layers.Input(shape=(28, 28, 1), name='encoder_input')\n",
    "    \n",
    "    # Bloc convolutif 1\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1')(encoder_input)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same', name='pool1')(x)  # 14x14\n",
    "    \n",
    "    # Bloc convolutif 2\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same', name='pool2')(x)  # 7x7\n",
    "    \n",
    "    # Aplatissement et compression vers l'espace latent\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    latent = layers.Dense(latent_dim, activation='relu', name='latent_space')(x)\n",
    "    \n",
    "    encoder = Model(encoder_input, latent, name='conv_encoder')\n",
    "    \n",
    "    # === DECODER ===\n",
    "    decoder_input = layers.Input(shape=(latent_dim,), name='decoder_input')\n",
    "    \n",
    "    # Expansion et reshape\n",
    "    x = layers.Dense(7 * 7 * 64, activation='relu', name='dense_decoder')(decoder_input)\n",
    "    x = layers.Reshape((7, 7, 64), name='reshape')(x)\n",
    "    \n",
    "    # D√©convolution 1 (upsampling)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', strides=2, \n",
    "                               padding='same', name='deconv1')(x)  # 14x14\n",
    "    \n",
    "    # D√©convolution 2 (upsampling)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=2, \n",
    "                               padding='same', name='deconv2')(x)  # 28x28\n",
    "    \n",
    "    # Couche de sortie\n",
    "    decoder_output = layers.Conv2D(1, (3, 3), activation='sigmoid', \n",
    "                                   padding='same', name='output')(x)\n",
    "    \n",
    "    decoder = Model(decoder_input, decoder_output, name='conv_decoder')\n",
    "    \n",
    "    # === AUTOENCODER COMPLET ===\n",
    "    autoencoder_input = layers.Input(shape=(28, 28, 1), name='autoencoder_input')\n",
    "    encoded = encoder(autoencoder_input)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = Model(autoencoder_input, decoded, name='conv_autoencoder')\n",
    "    \n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# Cr√©ation du mod√®le\n",
    "conv_latent_dim = 64\n",
    "conv_autoencoder, conv_encoder, conv_decoder = create_convolutional_autoencoder(conv_latent_dim)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ARCHITECTURE DE L'AUTO-ENCODEUR CONVOLUTIF\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüîπ ENCODER:\")\n",
    "conv_encoder.summary()\n",
    "print(\"\\nüîπ DECODER:\")\n",
    "conv_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Comparaison Dense vs Convolutif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptage des param√®tres\n",
    "dense_total = dense_autoencoder.count_params()\n",
    "conv_total = conv_autoencoder.count_params()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Comparaison du nombre de param√®tres\n",
    "models = ['Auto-encodeur\\nDense', 'Auto-encodeur\\nConvolutif']\n",
    "params = [dense_total, conv_total]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "bars = ax1.bar(models, params, color=colors, edgecolor='black', linewidth=2, width=0.6)\n",
    "ax1.set_ylabel('Nombre de param√®tres', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Comparaison du nombre de param√®tres', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, param in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{param:,}\\nparam√®tres',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Comparaison des dimensions latentes\n",
    "latent_dims = ['Dense\\n(latent=32)', 'Convolutif\\n(latent=64)']\n",
    "compression_ratios = [784/32, 784/64]\n",
    "bars2 = ax2.bar(latent_dims, compression_ratios, color=colors, \n",
    "               edgecolor='black', linewidth=2, width=0.6)\n",
    "ax2.set_ylabel('Taux de compression (√ó)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Taux de compression', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, ratio in zip(bars2, compression_ratios):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'√ó{ratio:.1f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä Dense      : {dense_total:,} param√®tres | Latent: 32 dims\")\n",
    "print(f\"üìä Convolutif : {conv_total:,} param√®tres | Latent: 64 dims\")\n",
    "if conv_total < dense_total:\n",
    "    print(f\"\\n‚úÖ Le CNN a {dense_total - conv_total:,} param√®tres de moins !\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Le CNN a {conv_total - dense_total:,} param√®tres de plus\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Compilation et entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "conv_autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['mse']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Auto-encodeur convolutif compil√©\")\n",
    "\n",
    "# Callback de visualisation\n",
    "viz_callback_conv = AutoencoderVisualizationCallback(X_val_cnn, n_images=8)\n",
    "\n",
    "print(\"\\nüöÄ D√©but de l'entra√Ænement de l'auto-encodeur CONVOLUTIF...\\n\")\n",
    "\n",
    "# Entra√Ænement\n",
    "history_conv = conv_autoencoder.fit(\n",
    "    X_train_cnn, X_train_cnn,\n",
    "    validation_data=(X_val_cnn, X_val_cnn),\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    callbacks=[viz_callback_conv],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 √âvaluation et comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation\n",
    "conv_test_loss, conv_test_mse = conv_autoencoder.evaluate(X_test_cnn, X_test_cnn, verbose=0)\n",
    "\n",
    "# Reconstruction d'exemples\n",
    "n_examples = 10\n",
    "test_sample_cnn = X_test_cnn[:n_examples]\n",
    "reconstructed_conv = conv_autoencoder.predict(test_sample_cnn, verbose=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä COMPARAISON FINALE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dense      - Loss: {test_loss:.6f} | MSE: {test_mse:.6f} | RMSE: {np.sqrt(test_mse):.6f}\")\n",
    "print(f\"Convolutif - Loss: {conv_test_loss:.6f} | MSE: {conv_test_mse:.6f} | RMSE: {np.sqrt(conv_test_mse):.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if conv_test_mse < test_mse:\n",
    "    improvement = ((test_mse - conv_test_mse) / test_mse) * 100\n",
    "    print(f\"\\nüèÜ Le CNN est meilleur de {improvement:.2f}% !\")\n",
    "else:\n",
    "    print(f\"\\nüèÜ Le Dense est meilleur !\")\n",
    "\n",
    "# Visualisation comparative\n",
    "fig, axes = plt.subplots(4, n_examples, figsize=(20, 8))\n",
    "fig.suptitle('Comparaison Dense vs Convolutif', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Pr√©dictions Dense\n",
    "reconstructed_dense = dense_autoencoder.predict(X_test_flat[:n_examples], verbose=0)\n",
    "\n",
    "for i in range(n_examples):\n",
    "    # Original\n",
    "    axes[0, i].imshow(X_test[i], cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Original', fontsize=12, fontweight='bold')\n",
    "    axes[0, i].set_title(class_names[y_test[i]], fontsize=9)\n",
    "    \n",
    "    # Dense\n",
    "    axes[1, i].imshow(reconstructed_dense[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Dense', fontsize=12, fontweight='bold')\n",
    "    mse_dense = np.mean((X_test_flat[i] - reconstructed_dense[i]) ** 2)\n",
    "    axes[1, i].set_xlabel(f'MSE: {mse_dense:.4f}', fontsize=8)\n",
    "    \n",
    "    # Convolutif\n",
    "    axes[2, i].imshow(reconstructed_conv[i].reshape(28, 28), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel('Convolutif', fontsize=12, fontweight='bold')\n",
    "    mse_conv = np.mean((X_test_cnn[i] - reconstructed_conv[i]) ** 2)\n",
    "    axes[2, i].set_xlabel(f'MSE: {mse_conv:.4f}', fontsize=8)\n",
    "    \n",
    "    # Diff√©rence\n",
    "    diff = np.abs(reconstructed_conv[i].reshape(28, 28) - reconstructed_dense[i].reshape(28, 28))\n",
    "    axes[3, i].imshow(diff, cmap='RdYlGn_r', vmin=0, vmax=0.5)\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_ylabel('Diff Conv-Dense', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 3 : DENOISING AUTO-ENCODEUR\n",
    "\n",
    "## 6. D√©bruitage d'images\n",
    "\n",
    "### Principe\n",
    "\n",
    "Un **denoising autoencoder** apprend √† reconstruire des images propres √† partir d'images bruit√©es :\n",
    "\n",
    "```\n",
    "Image propre ‚Üí Ajout de bruit ‚Üí Image bruit√©e ‚Üí Encoder ‚Üí Decoder ‚Üí Image reconstruite (propre)\n",
    "```\n",
    "\n",
    "### Applications\n",
    "- üì∑ Restauration de vieilles photos\n",
    "- üé• Am√©lioration de vid√©os basse qualit√©\n",
    "- üî¨ D√©bruitage d'images m√©dicales\n",
    "- üõ∞Ô∏è Am√©lioration d'images satellites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Cr√©ation de donn√©es bruit√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(images, noise_factor=0.3):\n",
    "    \"\"\"\n",
    "    Ajoute du bruit gaussien aux images.\n",
    "    \n",
    "    Args:\n",
    "        images: Images normalis√©es [0, 1]\n",
    "        noise_factor: Intensit√© du bruit\n",
    "    \n",
    "    Returns:\n",
    "        Images bruit√©es, clipp√©es entre [0, 1]\n",
    "    \"\"\"\n",
    "    noisy_images = images + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=images.shape\n",
    "    )\n",
    "    noisy_images = np.clip(noisy_images, 0.0, 1.0)\n",
    "    return noisy_images\n",
    "\n",
    "# Cr√©ation des donn√©es bruit√©es\n",
    "noise_factor = 0.4\n",
    "\n",
    "X_train_noisy = add_noise(X_train_cnn, noise_factor)\n",
    "X_val_noisy = add_noise(X_val_cnn, noise_factor)\n",
    "X_test_noisy = add_noise(X_test_cnn, noise_factor)\n",
    "\n",
    "print(f\"‚úÖ Bruit ajout√© avec facteur = {noise_factor}\")\n",
    "print(f\"   Forme des donn√©es bruit√©es: {X_train_noisy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Visualisation du bruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'effet du bruit\n",
    "n_samples = 10\n",
    "\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(20, 5))\n",
    "fig.suptitle(f'Effet du bruit (facteur = {noise_factor})', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Image originale\n",
    "    axes[0, i].imshow(X_test_cnn[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Originale', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Image bruit√©e\n",
    "    axes[1, i].imshow(X_test_noisy[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Bruit√©e', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Entra√Ænement du denoising autoencoder\n",
    "\n",
    "**Note importante** : Nous utilisons la m√™me architecture que l'auto-encodeur convolutif, mais avec un entra√Ænement diff√©rent :\n",
    "- **Input** : Images bruit√©es\n",
    "- **Output cible** : Images propres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation d'un nouveau mod√®le (m√™me architecture)\n",
    "denoising_autoencoder, denoising_encoder, denoising_decoder = create_convolutional_autoencoder(conv_latent_dim)\n",
    "\n",
    "# Compilation\n",
    "denoising_autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['mse']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Denoising auto-encodeur cr√©√© et compil√©\")\n",
    "\n",
    "# Callback personnalis√© pour denoising\n",
    "class DenoisingVisualizationCallback(Callback):\n",
    "    def __init__(self, noisy_data, clean_data, n_images=8):\n",
    "        super().__init__()\n",
    "        self.noisy_data = noisy_data[:n_images]\n",
    "        self.clean_data = clean_data[:n_images]\n",
    "        self.n_images = n_images\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epochs.append(epoch + 1)\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        gs = fig.add_gridspec(4, self.n_images + 1, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Graphique de la loss\n",
    "        ax_loss = fig.add_subplot(gs[:, 0])\n",
    "        ax_loss.plot(self.epochs, self.loss, 'o-', label='Loss Train', \n",
    "                    linewidth=2.5, markersize=8, color='#2E86AB')\n",
    "        ax_loss.plot(self.epochs, self.val_loss, 's-', label='Loss Validation', \n",
    "                    linewidth=2.5, markersize=8, color='#A23B72')\n",
    "        ax_loss.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "        ax_loss.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "        ax_loss.set_title('√âvolution de la Loss', fontsize=14, fontweight='bold')\n",
    "        ax_loss.legend(fontsize=10)\n",
    "        ax_loss.grid(alpha=0.3)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        denoised = self.model.predict(self.noisy_data, verbose=0)\n",
    "        \n",
    "        # Affichage\n",
    "        for i in range(self.n_images):\n",
    "            # Original propre\n",
    "            ax = fig.add_subplot(gs[0, i + 1])\n",
    "            ax.imshow(self.clean_data[i].reshape(28, 28), cmap='gray')\n",
    "            if i == 0:\n",
    "                ax.set_ylabel('Original\\n(propre)', fontsize=11, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # Bruit√©e (input)\n",
    "            ax = fig.add_subplot(gs[1, i + 1])\n",
    "            ax.imshow(self.noisy_data[i].reshape(28, 28), cmap='gray')\n",
    "            if i == 0:\n",
    "                ax.set_ylabel('Bruit√©e\\n(input)', fontsize=11, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # D√©bruit√©e (output)\n",
    "            ax = fig.add_subplot(gs[2, i + 1])\n",
    "            ax.imshow(denoised[i].reshape(28, 28), cmap='gray')\n",
    "            if i == 0:\n",
    "                ax.set_ylabel('D√©bruit√©e\\n(output)', fontsize=11, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            \n",
    "            # Erreur vs original\n",
    "            ax = fig.add_subplot(gs[3, i + 1])\n",
    "            diff = np.abs(self.clean_data[i].reshape(28, 28) - denoised[i].reshape(28, 28))\n",
    "            ax.imshow(diff, cmap='hot', vmin=0, vmax=1)\n",
    "            if i == 0:\n",
    "                ax.set_ylabel('Erreur', fontsize=11, fontweight='bold')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "        plt.suptitle(f'Denoising Auto-encodeur - Epoch {epoch + 1}/{self.params[\"epochs\"]}', \n",
    "                    fontsize=16, fontweight='bold', y=0.98)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']}\")\n",
    "        print(f\"Loss: {logs.get('loss'):.6f} - Val Loss: {logs.get('val_loss'):.6f}\")\n",
    "\n",
    "# Callback\n",
    "denoising_viz_callback = DenoisingVisualizationCallback(X_val_noisy, X_val_cnn, n_images=8)\n",
    "\n",
    "print(\"\\nüöÄ D√©but de l'entra√Ænement du DENOISING auto-encodeur...\\n\")\n",
    "\n",
    "# Entra√Ænement: INPUT = bruit√©e, OUTPUT = propre\n",
    "history_denoising = denoising_autoencoder.fit(\n",
    "    X_train_noisy, X_train_cnn,  # Input bruit√©e, output propre\n",
    "    validation_data=(X_val_noisy, X_val_cnn),\n",
    "    epochs=25,\n",
    "    batch_size=256,\n",
    "    callbacks=[denoising_viz_callback],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 √âvaluation du denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation\n",
    "denoising_test_loss, denoising_test_mse = denoising_autoencoder.evaluate(\n",
    "    X_test_noisy, X_test_cnn, verbose=0\n",
    ")\n",
    "\n",
    "# D√©bruitage d'exemples\n",
    "n_examples = 15\n",
    "test_noisy_sample = X_test_noisy[:n_examples]\n",
    "test_clean_sample = X_test_cnn[:n_examples]\n",
    "denoised_images = denoising_autoencoder.predict(test_noisy_sample, verbose=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä R√âSULTATS - DENOISING AUTO-ENCODEUR\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Loss sur le test: {denoising_test_loss:.6f}\")\n",
    "print(f\"MSE sur le test: {denoising_test_mse:.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calcul des m√©triques de qualit√©\n",
    "# PSNR (Peak Signal-to-Noise Ratio) - Plus c'est √©lev√©, mieux c'est\n",
    "def calculate_psnr(original, reconstructed):\n",
    "    mse = np.mean((original - reconstructed) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    max_pixel = 1.0\n",
    "    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "psnr_noisy = calculate_psnr(test_clean_sample, test_noisy_sample)\n",
    "psnr_denoised = calculate_psnr(test_clean_sample, denoised_images)\n",
    "\n",
    "print(f\"\\nüìä PSNR (Peak Signal-to-Noise Ratio):\")\n",
    "print(f\"   Images bruit√©es:  {psnr_noisy:.2f} dB\")\n",
    "print(f\"   Images d√©bruit√©es: {psnr_denoised:.2f} dB\")\n",
    "print(f\"   Am√©lioration: {psnr_denoised - psnr_noisy:.2f} dB\")\n",
    "\n",
    "# Visualisation d√©taill√©e\n",
    "fig, axes = plt.subplots(4, n_examples, figsize=(22, 8))\n",
    "fig.suptitle('Performance du Denoising Auto-encodeur', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(n_examples):\n",
    "    # Original propre\n",
    "    axes[0, i].imshow(test_clean_sample[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Original\\n(propre)', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Bruit√©e\n",
    "    axes[1, i].imshow(test_noisy_sample[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Bruit√©e', fontsize=11, fontweight='bold')\n",
    "    # MSE avec original\n",
    "    mse_noisy = np.mean((test_clean_sample[i] - test_noisy_sample[i]) ** 2)\n",
    "    axes[1, i].set_xlabel(f'MSE: {mse_noisy:.4f}', fontsize=8, color='red')\n",
    "    \n",
    "    # D√©bruit√©e\n",
    "    axes[2, i].imshow(denoised_images[i].reshape(28, 28), cmap='gray')\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel('D√©bruit√©e', fontsize=11, fontweight='bold')\n",
    "    # MSE avec original\n",
    "    mse_denoised = np.mean((test_clean_sample[i] - denoised_images[i]) ** 2)\n",
    "    axes[2, i].set_xlabel(f'MSE: {mse_denoised:.4f}', fontsize=8, color='green')\n",
    "    \n",
    "    # Erreur r√©siduelle\n",
    "    diff = np.abs(test_clean_sample[i].reshape(28, 28) - denoised_images[i].reshape(28, 28))\n",
    "    axes[3, i].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_ylabel('Erreur\\nr√©siduelle', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Le denoising auto-encodeur r√©duit significativement le bruit !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Applications pratiques des auto-encodeurs\n",
    "\n",
    "### 7.1 D√©tection d'anomalies\n",
    "\n",
    "Les auto-encodeurs sont excellents pour d√©tecter des anomalies :\n",
    "- Entra√Æner sur des donn√©es \"normales\"\n",
    "- Les anomalies produisent une erreur de reconstruction √©lev√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation de d√©tection d'anomalies\n",
    "# Entra√Ænons un auto-encodeur uniquement sur les T-shirts (classe 0)\n",
    "\n",
    "print(\"üîç Simulation de d√©tection d'anomalies...\")\n",
    "print(\"   Concept: Entra√Æner uniquement sur une classe, d√©tecter les autres\\n\")\n",
    "\n",
    "# Filtrage: uniquement les T-shirts pour l'entra√Ænement\n",
    "tshirt_mask = y_train_final == 0\n",
    "X_train_tshirts = X_train_cnn[tshirt_mask][:5000]  # 5000 T-shirts\n",
    "\n",
    "print(f\"üì¶ Entra√Ænement sur {len(X_train_tshirts)} T-shirts uniquement\")\n",
    "\n",
    "# Cr√©ation et entra√Ænement rapide\n",
    "anomaly_ae, _, _ = create_convolutional_autoencoder(latent_dim=32)\n",
    "anomaly_ae.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "print(\"‚è≥ Entra√Ænement rapide (10 epochs)...\")\n",
    "anomaly_ae.fit(\n",
    "    X_train_tshirts, X_train_tshirts,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=0\n",
    ")\n",
    "print(\"‚úÖ Entra√Ænement termin√©\\n\")\n",
    "\n",
    "# Test sur diff√©rentes classes\n",
    "classes_to_test = [0, 1, 2, 5, 9]  # T-shirt, Pantalon, Pull, Sandale, Bottine\n",
    "reconstruction_errors = {}\n",
    "\n",
    "for class_idx in classes_to_test:\n",
    "    # S√©lection de 100 exemples de cette classe\n",
    "    mask = y_test == class_idx\n",
    "    X_class = X_test_cnn[mask][:100]\n",
    "    \n",
    "    # Reconstruction\n",
    "    reconstructed = anomaly_ae.predict(X_class, verbose=0)\n",
    "    \n",
    "    # Calcul de l'erreur de reconstruction\n",
    "    mse_per_image = np.mean((X_class - reconstructed) ** 2, axis=(1, 2, 3))\n",
    "    reconstruction_errors[class_names[class_idx]] = mse_per_image\n",
    "\n",
    "# Visualisation des erreurs de reconstruction\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Boxplot\n",
    "ax1.boxplot(\n",
    "    [reconstruction_errors[class_names[i]] for i in classes_to_test],\n",
    "    labels=[class_names[i] for i in classes_to_test],\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightblue', edgecolor='black', linewidth=1.5),\n",
    "    medianprops=dict(color='red', linewidth=2)\n",
    ")\n",
    "ax1.set_ylabel('Erreur de reconstruction (MSE)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution des erreurs par classe\\n(Entra√Æn√© uniquement sur T-shirts)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Moyennes\n",
    "means = [np.mean(reconstruction_errors[class_names[i]]) for i in classes_to_test]\n",
    "colors_bar = ['green' if i == 0 else 'red' for i in classes_to_test]\n",
    "bars = ax2.bar([class_names[i] for i in classes_to_test], means, \n",
    "               color=colors_bar, edgecolor='black', linewidth=2, alpha=0.7)\n",
    "ax2.set_ylabel('Erreur moyenne de reconstruction', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Erreur moyenne par classe\\n(Vert = classe normale, Rouge = anomalies)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for bar, mean in zip(bars, means):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{mean:.5f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä ANALYSE:\")\n",
    "print(f\"   Erreur moyenne sur T-shirts (normal): {means[0]:.5f}\")\n",
    "print(f\"   Erreur moyenne sur autres classes: {np.mean(means[1:]):.5f}\")\n",
    "print(f\"   Ratio d'augmentation: √ó{np.mean(means[1:]) / means[0]:.2f}\")\n",
    "print(\"\\nüí° Les anomalies (classes non vues) ont une erreur beaucoup plus √©lev√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Exploration de l'espace latent - G√©n√©ration d'images\n",
    "\n",
    "Nous pouvons explorer l'espace latent pour g√©n√©rer de nouvelles images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage de quelques images de r√©f√©rence\n",
    "print(\"üé® Exploration de l'espace latent...\\n\")\n",
    "\n",
    "# S√©lection de 2 images diff√©rentes\n",
    "idx1, idx2 = 42, 100\n",
    "img1 = X_test_cnn[idx1:idx1+1]\n",
    "img2 = X_test_cnn[idx2:idx2+1]\n",
    "\n",
    "# Encodage\n",
    "z1 = conv_encoder.predict(img1, verbose=0)\n",
    "z2 = conv_encoder.predict(img2, verbose=0)\n",
    "\n",
    "# Interpolation lin√©aire dans l'espace latent\n",
    "n_steps = 10\n",
    "alphas = np.linspace(0, 1, n_steps)\n",
    "interpolated_images = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Interpolation: z = (1-Œ±)*z1 + Œ±*z2\n",
    "    z_interpolated = (1 - alpha) * z1 + alpha * z2\n",
    "    # D√©codage\n",
    "    img_interpolated = conv_decoder.predict(z_interpolated, verbose=0)\n",
    "    interpolated_images.append(img_interpolated[0])\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, n_steps, figsize=(20, 5))\n",
    "fig.suptitle('Interpolation dans l\\'espace latent', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Images originales et interpol√©es\n",
    "for i, alpha in enumerate(alphas):\n",
    "    axes[0, i].imshow(interpolated_images[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title(f'Œ± = {alpha:.1f}', fontsize=10)\n",
    "    \n",
    "    # Marquer les images de d√©part et d'arriv√©e\n",
    "    if i == 0:\n",
    "        axes[0, i].set_xlabel('Image 1\\n(d√©part)', fontsize=9, color='blue', fontweight='bold')\n",
    "    elif i == n_steps - 1:\n",
    "        axes[0, i].set_xlabel('Image 2\\n(arriv√©e)', fontsize=9, color='red', fontweight='bold')\n",
    "\n",
    "# Deuxi√®me ligne: zoom sur le milieu de l'interpolation\n",
    "middle_indices = [2, 3, 4, 5, 6, 7]\n",
    "for i, idx in enumerate(middle_indices):\n",
    "    axes[1, i+2].imshow(interpolated_images[idx].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i+2].axis('off')\n",
    "    axes[1, i+2].set_title(f'Œ± = {alphas[idx]:.2f}', fontsize=9)\n",
    "\n",
    "# Masquer les axes inutilis√©s\n",
    "for i in [0, 1, 8, 9]:\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[1, 0].text(0.5, 0.5, 'Zoom sur\\nla transition', \n",
    "               ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° OBSERVATION:\")\n",
    "print(\"   L'interpolation dans l'espace latent produit des transitions douces\")\n",
    "print(\"   entre les deux images, montrant que l'espace est bien structur√© !\")\n",
    "print(f\"\\n   Image 1: {class_names[y_test[idx1]]}\")\n",
    "print(f\"   Image 2: {class_names[y_test[idx2]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 G√©n√©ration al√©atoire depuis l'espace latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration d'images al√©atoires\n",
    "print(\"üé≤ G√©n√©ration d'images al√©atoires depuis l'espace latent...\\n\")\n",
    "\n",
    "# Analyse de la distribution de l'espace latent\n",
    "all_latent = conv_encoder.predict(X_test_cnn[:1000], verbose=0)\n",
    "latent_mean = np.mean(all_latent, axis=0)\n",
    "latent_std = np.std(all_latent, axis=0)\n",
    "\n",
    "print(f\"üìä Statistiques de l'espace latent (sur 1000 exemples):\")\n",
    "print(f\"   Moyenne: {latent_mean.mean():.4f} ¬± {latent_mean.std():.4f}\")\n",
    "print(f\"   √âcart-type: {latent_std.mean():.4f} ¬± {latent_std.std():.4f}\\n\")\n",
    "\n",
    "# G√©n√©ration de codes latents al√©atoires\n",
    "n_random = 20\n",
    "random_latents = np.random.normal(latent_mean, latent_std, size=(n_random, conv_latent_dim))\n",
    "\n",
    "# D√©codage\n",
    "random_images = conv_decoder.predict(random_latents, verbose=0)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 10, figsize=(20, 5))\n",
    "fig.suptitle('Images g√©n√©r√©es al√©atoirement depuis l\\'espace latent', \n",
    "            fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(n_random):\n",
    "    row = i // 10\n",
    "    col = i % 10\n",
    "    axes[row, col].imshow(random_images[i].reshape(28, 28), cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° OBSERVATION:\")\n",
    "print(\"   Certaines images ressemblent √† des v√™tements r√©alistes,\")\n",
    "print(\"   d'autres sont plus floues ou abstraites.\")\n",
    "print(\"   ‚ö†Ô∏è  Pour une meilleure g√©n√©ration, utilisez des VAE (Variational Autoencoders) !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Conclusion et Comparaison Finale\n",
    "\n",
    "### 8.1 Tableau r√©capitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cr√©ation du tableau comparatif\n",
    "comparison_data = {\n",
    "    'Mod√®le': ['Dense AE', 'Conv AE', 'Denoising AE'],\n",
    "    'Architecture': ['Dense', 'Convolutif', 'Convolutif'],\n",
    "    'Param√®tres': [\n",
    "        f\"{dense_autoencoder.count_params():,}\",\n",
    "        f\"{conv_autoencoder.count_params():,}\",\n",
    "        f\"{denoising_autoencoder.count_params():,}\"\n",
    "    ],\n",
    "    'Latent Dim': [32, 64, 64],\n",
    "    'MSE Test': [\n",
    "        f\"{test_mse:.6f}\",\n",
    "        f\"{conv_test_mse:.6f}\",\n",
    "        f\"{denoising_test_mse:.6f}\"\n",
    "    ],\n",
    "    'Cas d\\'usage': [\n",
    "        'G√©n√©ral, simple',\n",
    "        'Images, meilleure qualit√©',\n",
    "        'D√©bruitage, robustesse'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìä COMPARAISON FINALE DES AUTO-ENCODEURS\")\n",
    "print(\"=\"*90)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Visualisation finale\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('R√©capitulatif des performances', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Graphique 1: Courbes de loss\n",
    "ax = axes[0, 0]\n",
    "epochs = range(1, 21)\n",
    "ax.plot(epochs, history_dense.history['val_loss'], 'o-', label='Dense AE', linewidth=2)\n",
    "ax.plot(epochs, history_conv.history['val_loss'], 's-', label='Conv AE', linewidth=2)\n",
    "ax.plot(range(1, 26), history_denoising.history['val_loss'], '^-', label='Denoising AE', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Validation Loss', fontsize=11, fontweight='bold')\n",
    "ax.set_title('√âvolution de la Loss', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Graphique 2: Comparaison param√®tres\n",
    "ax = axes[0, 1]\n",
    "models = ['Dense AE', 'Conv AE', 'Denoising AE']\n",
    "params_list = [\n",
    "    dense_autoencoder.count_params(),\n",
    "    conv_autoencoder.count_params(),\n",
    "    denoising_autoencoder.count_params()\n",
    "]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "bars = ax.bar(models, params_list, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Nombre de param√®tres', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Complexit√© des mod√®les', fontsize=13, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=15)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, p in zip(bars, params_list):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{p:,}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Graphique 3: MSE finale\n",
    "ax = axes[0, 2]\n",
    "mse_list = [test_mse, conv_test_mse, denoising_test_mse]\n",
    "bars = ax.bar(models, mse_list, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('MSE sur test', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Qualit√© de reconstruction', fontsize=13, fontweight='bold')\n",
    "ax.tick_params(axis='x', rotation=15)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, mse in zip(bars, mse_list):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{mse:.5f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Exemples de reconstruction - Dense\n",
    "ax = axes[1, 0]\n",
    "test_idx = 5\n",
    "recon_dense = dense_autoencoder.predict(X_test_flat[test_idx:test_idx+1], verbose=0)\n",
    "comparison = np.hstack([X_test[test_idx], recon_dense.reshape(28, 28)])\n",
    "ax.imshow(comparison, cmap='gray')\n",
    "ax.set_title('Dense AE\\nOriginal | Reconstruit', fontsize=11, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# Exemples de reconstruction - Conv\n",
    "ax = axes[1, 1]\n",
    "recon_conv = conv_autoencoder.predict(X_test_cnn[test_idx:test_idx+1], verbose=0)\n",
    "comparison = np.hstack([X_test[test_idx], recon_conv.reshape(28, 28)])\n",
    "ax.imshow(comparison, cmap='gray')\n",
    "ax.set_title('Conv AE\\nOriginal | Reconstruit', fontsize=11, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# Exemples de d√©bruitage\n",
    "ax = axes[1, 2]\n",
    "denoised = denoising_autoencoder.predict(X_test_noisy[test_idx:test_idx+1], verbose=0)\n",
    "comparison = np.hstack([X_test_noisy[test_idx].reshape(28, 28), denoised.reshape(28, 28)])\n",
    "ax.imshow(comparison, cmap='gray')\n",
    "ax.set_title('Denoising AE\\nBruit√©e | D√©bruit√©e', fontsize=11, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Points cl√©s √† retenir\n",
    "\n",
    "#### üéØ Architecture\n",
    "- **Dense AE** : Simple, bon point de d√©part\n",
    "- **Conv AE** : Meilleur pour les images, moins de param√®tres\n",
    "- **Denoising AE** : Robuste, apprend des repr√©sentations plus riches\n",
    "\n",
    "#### üìä Performance\n",
    "- Les CNN produisent des reconstructions plus nettes\n",
    "- Le denoising am√©liore la qualit√© des features apprises\n",
    "- L'espace latent capture les caract√©ristiques essentielles\n",
    "\n",
    "#### üí° Applications\n",
    "1. **Compression** : R√©duire la taille des donn√©es\n",
    "2. **D√©bruitage** : Nettoyer des images bruit√©es\n",
    "3. **D√©tection d'anomalies** : Identifier des donn√©es inhabituelles\n",
    "4. **G√©n√©ration** : Cr√©er de nouvelles donn√©es (avec VAE)\n",
    "5. **Pre-training** : Initialiser des r√©seaux pour d'autres t√¢ches\n",
    "6. **Feature extraction** : Extraire des repr√©sentations utiles\n",
    "\n",
    "#### ‚öôÔ∏è Hyperparam√®tres importants\n",
    "- **Dimension latente** : Trade-off compression vs qualit√©\n",
    "- **Architecture** : Plus profonde = meilleures features\n",
    "- **Fonction de loss** : MSE pour pixel par pixel, BCE pour probabilit√©s\n",
    "- **Dropout** : R√©gularisation, √©vite l'overfitting\n",
    "\n",
    "### üöÄ Pour aller plus loin\n",
    "\n",
    "1. **VAE (Variational Autoencoder)** : G√©n√©ration probabiliste\n",
    "2. **Sparse Autoencoder** : R√©gularisation par sparsit√©\n",
    "3. **Contractive Autoencoder** : Robustesse aux perturbations\n",
    "4. **Adversarial Autoencoder** : Combinaison avec GAN\n",
    "5. **U-Net** : Architecture encoder-decoder pour segmentation\n",
    "6. **Transformer Autoencoder** : Avec m√©canisme d'attention\n",
    "\n",
    "### üìö Ressources\n",
    "- [Auto-Encoding Variational Bayes (Kingma & Welling, 2013)](https://arxiv.org/abs/1312.6114)\n",
    "- [Denoising Autoencoders (Vincent et al., 2008)](https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf)\n",
    "- [Keras Autoencoders Tutorial](https://blog.keras.io/building-autoencoders-in-keras.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercices pratiques\n",
    "\n",
    "Pour approfondir votre compr√©hension, voici quelques exercices :\n",
    "\n",
    "### Exercice 1 : Modifier la dimension latente\n",
    "- Testez diff√©rentes dimensions latentes (8, 16, 64, 128)\n",
    "- Observez l'impact sur la qualit√© de reconstruction\n",
    "- Visualisez le trade-off compression vs qualit√©\n",
    "\n",
    "### Exercice 2 : Architecture plus profonde\n",
    "- Ajoutez des couches convolutives suppl√©mentaires\n",
    "- Utilisez Batch Normalization\n",
    "- Comparez les performances\n",
    "\n",
    "### Exercice 3 : Autres types de bruit\n",
    "- Testez du bruit salt-and-pepper (pixels al√©atoires noirs ou blancs)\n",
    "- Ajoutez du bruit par blocs (masquer des r√©gions)\n",
    "- √âvaluez la robustesse du denoising AE\n",
    "\n",
    "### Exercice 4 : Transfer learning\n",
    "- Utilisez l'encoder pr√©-entra√Æn√© pour une t√¢che de classification\n",
    "- Comparez avec un entra√Ænement from scratch\n",
    "- Mesurez le gain de performance\n",
    "\n",
    "### Exercice 5 : Autres datasets\n",
    "- Appliquez les auto-encodeurs sur MNIST (chiffres)\n",
    "- Testez sur CIFAR-10 (images couleur 32√ó32)\n",
    "- Adaptez l'architecture si n√©cessaire\n",
    "\n",
    "### Exercice 6 : VAE simple\n",
    "- Impl√©mentez un Variational Autoencoder basique\n",
    "- Ajoutez une loss KL-divergence\n",
    "- G√©n√©rez de nouvelles images en √©chantillonnant\n",
    "\n",
    "**Bon apprentissage ! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
