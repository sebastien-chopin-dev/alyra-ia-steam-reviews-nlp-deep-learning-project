{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2aebbe",
   "metadata": {},
   "source": [
    "Hyperparamètres clés en Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f205d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemples d'hyperparamètres critiques\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"batch_size\": [16, 32, 64, 128],\n",
    "    \"epochs\": [50, 100, 200],\n",
    "    \"optimizer\": [\"adam\", \"sgd\", \"rmsprop\"],\n",
    "    \"hidden_layers\": [1, 2, 3, 4],\n",
    "    \"neurons_per_layer\": [64, 128, 256, 512],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40998b0d",
   "metadata": {},
   "source": [
    "Grid Search - Recherche exhaustive Test de TOUTES les combinaisons possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a2ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "def create_model(learning_rate=0.001, hidden_units=128):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(hidden_units, activation=\"relu\", input_shape=(784,)),\n",
    "            Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Configuration Grid Search\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"hidden_units\": [64, 128, 256],\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"epochs\": [50, 100],\n",
    "}\n",
    "grid = GridSearchCV(\n",
    "    estimator=model, param_grid=param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7a727",
   "metadata": {},
   "source": [
    "Random Search - Recherche aléatoire - Souvent 90% des performances du Grid Search en 10% du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Distribution des hyperparamètres\n",
    "param_distributions = {\n",
    "    \"learning_rate\": uniform(0.0001, 0.1),  # Uniforme entre 0.0001 et 0.1001\n",
    "    \"hidden_units\": randint(50, 500),\n",
    "    \"batch_size\": [16, 32, 64, 128],\n",
    "    \"dropout_rate\": uniform(0.1, 0.4),\n",
    "    \"epochs\": randint(50, 200),\n",
    "}\n",
    "\n",
    "# Choix discret\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # 50 combinaisons aléatoires\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    random_state=42,\n",
    ")\n",
    "# Entier entre 50 et 499\n",
    "# Entre 0.1 et 0.5\n",
    "random_result = random_search.fit(X_train, y_train)\n",
    "print(f\"Meilleur score: {random_result.best_score_}\")\n",
    "print(f\"Meilleurs paramètres: {random_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630241a",
   "metadata": {},
   "source": [
    "Bayesian Optimization - Recherche intelligente - Converge plus rapidement vers l'optimum global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef934828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import optuna\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Définition de l'espace de recherche\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    n_units = trial.suggest_int(\"n_units\", 32, 512, step=32)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "\n",
    "    # Construction du modèle\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_units, activation=\"relu\", input_shape=(784,)))\n",
    "\n",
    "    for i in range(n_layers - 1):\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(n_units, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Entraînement avec early stopping\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[EarlyStopping(patience=10)],\n",
    "    )\n",
    "    # Retour de la métrique à optimiser\n",
    "    return max(history.history[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "# Lancement de l'optimisation\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(f\"Meilleure valeur: {study.best_value}\")\n",
    "print(f\"Meilleurs paramètres: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651e1a1",
   "metadata": {},
   "source": [
    "Learning Rate Scheduling -  Decay exponentiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d85433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diminution exponentielle du learning rate\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=1000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# Formule mathématique appliquée\n",
    "# lr = initial_lr * decay_rate^(step / decay_steps)\n",
    "optimizer = Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786dc7a6",
   "metadata": {},
   "source": [
    " Learning Rate Scheduling - Cyclic Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcab321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oscillation entre lr_min et lr_max\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def cyclic_lr_schedule(epoch, lr):\n",
    "    cycle_length = 20\n",
    "    lr_min, lr_max = 0.001, 0.01\n",
    "    cycle = np.floor(1 + epoch / (2 * cycle_length))\n",
    "    x = np.abs(epoch / cycle_length - 2 * cycle + 1)\n",
    "    lr = lr_min + (lr_max - lr_min) * np.maximum(0, 1 - x)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cyclic_lr_schedule)\n",
    "model.fit(X_train, y_train, callbacks=[lr_scheduler], epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f37e4ab",
   "metadata": {},
   "source": [
    "Architecture Tuning - Détermination du nombre optimal de couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8335673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_layer_depth():\n",
    "    results = {}\n",
    "    for n_layers in range(1, 8):  # Test de 1 à 7 couches\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, activation=\"relu\", input_shape=(784,)))\n",
    "\n",
    "        # Ajout des couches cachées\n",
    "        for i in range(n_layers - 1):\n",
    "            model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "        model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train, validation_split=0.2, epochs=50, verbose=0\n",
    "        )\n",
    "\n",
    "        results[n_layers] = {\n",
    "            \"train_acc\": max(history.history[\"accuracy\"]),\n",
    "            \"val_acc\": max(history.history[\"val_accuracy\"]),\n",
    "            \"params\": model.count_params(),\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fdf7cd",
   "metadata": {},
   "source": [
    "Architecture Tuning -  Optimisation des tailles de filtres CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58256347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cnn_architecture():\n",
    "    architectures = [\n",
    "        {\"filters\": [32, 64], \"kernel_sizes\": [3, 3]},\n",
    "        {\"filters\": [32, 64, 128], \"kernel_sizes\": [3, 3, 3]},\n",
    "        {\"filters\": [64, 128, 256], \"kernel_sizes\": [5, 3, 3]},\n",
    "        {\"filters\": [16, 32, 64, 128], \"kernel_sizes\": [7, 5, 3, 3]},\n",
    "    ]\n",
    "    for i, arch in enumerate(architectures):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                arch[\"filters\"][0],\n",
    "                kernel_size=arch[\"kernel_sizes\"][0],\n",
    "                activation=\"relu\",\n",
    "                input_shape=(28, 28, 1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "        for j in range(1, len(arch[\"filters\"])):\n",
    "            model.add(\n",
    "                Conv2D(\n",
    "                    arch[\"filters\"][j],\n",
    "                    kernel_size=arch[\"kernel_sizes\"][j],\n",
    "                    activation=\"relu\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation=\"relu\"))\n",
    "        model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "        # Test de l'architecture\n",
    "        print(f\"Architecture {i+1}: {model.count_params()} paramètres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd1e04",
   "metadata": {},
   "source": [
    "Batch Size et Memory Management - Impact sur la convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de différentes tailles de batch\n",
    "batch_sizes = [8, 16, 32, 64, 128, 256]\n",
    "convergence_results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Test avec batch_size = {batch_size}\")\n",
    "    model = create_base_model()\n",
    "    # Ajustement du learning rate selon la règle empirique\n",
    "    # lr = base_lr * sqrt(batch_size / base_batch_size)\n",
    "    base_lr = 0.001\n",
    "    adjusted_lr = base_lr * np.sqrt(batch_size / 32)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=adjusted_lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    convergence_results[batch_size] = {\n",
    "        \"final_accuracy\": history.history[\"val_accuracy\"][-1],\n",
    "        \"training_time\": training_time,\n",
    "        \"memory_usage\": get_memory_usage(),  # Fonction custom\n",
    "        \"steps_per_epoch\": len(X_train) // batch_size,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
